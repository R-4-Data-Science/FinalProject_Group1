---
title: "Final Project"
output: html_document
date: "2023-11-30"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r, echo = FALSE}

data <- as.data.frame(read.csv("abalone.data"), header = FALSE)
colnames(data) <- c("Sex", "Length", "Diameter", "Height", "Whole_weight", "Shucked_weight", "Viscera_weight", "Shell_weight", "Rings")
head(data)
#remove infant columns so we have binary response variable
data <- data[data$Sex != "I", ]
data <- data[1:100, ]
#convert Male to 1 and Female to 0 
data$Sex <- ifelse(data$Sex == "M", 1,0)

y_abalone <- data$Sex
x_columns <- c("Length", "Diameter", "Height", "Whole_weight", "Shucked_weight", "Viscera_weight", "Shell_weight", "Rings")
X_abalone <- as.matrix(data[x_columns])
##add a column of ones for the intercept
X_abalone <- cbind(1, X_abalone)
X_abalone <- X_abalone[1:length(y_abalone), ]



```

## *Beta Loss function to prepare for logistic regression function*
```{r loss}
pf <- function(x, beta){
  out <- 1/(1 + exp(-t(x)%*%beta))
  return(out)
}


beta_ls <- function(beta, x, y){
  sum <- 0
  for(i in 1:nrow(x)){
    xi <- as.matrix(unlist(x[i,]), ncol = 1) #format it so dimensions work out later
    yi <- y[i]
    pi <- pf(xi,as.matrix(beta))


    res <- (-1*yi)*(log(pi)) - (1 - yi)*log(1-pi)
    sum <- sum + res
  }
  return(sum)

}
```

## *Logistic Regression with Optimization Function*
The initial values for optimization were obtained from the least-squares formula (XTX)−1XTy.

```{r logreg}
log_reg <- function(X, y){
  colnames(X)[1] <- "Intercept"
  B_initial <- solve(t(X)%*%X)%*%t(X)%*%y
  result <- optim(par = B_initial, fn = beta_ls, x = X, y = y)
  parameters <- result$par
  y_pred <- X%*%parameters
  out <- list("betas" = parameters, "y_pred" = y_pred, "y_actual" = y, "X" = X, "Parameters" = parameters)
  class(out) <- "my_b"

  return(out)

}

```

Example using Abalone Dataset
The response variable is the sex, male denoted as 1 and female denoted with 0. The predictors used are length, diameter, height, whole weight, shucked weight, viscera weight, shell weight and rings. The output from this function returns the estimated beta coefficients for each of the predictors, including the intercept. 
```{r abalone, echo = FALSE}
results <- log_reg(X_abalone, y_abalone)
results$betas

B_ab <- solve(t(X_abalone)%*%X_abalone)%*%t(X_abalone)%*%y_abalone
test <- optim(par = B_ab, fn = beta_ls, x = X_abalone, y = y_abalone)

test_pred <- X_abalone%*%test$par


```




### *Plot of the fitted logistic curve to the responses*

The y -axis is the binary response y while the x-axis represents a sequence of values from the range of Xβ̂. 

```{r logplot}
plot.my_b <- function(obj){
  y_pred <- obj$y_pred
  y_actual <- obj$y_actual
  X <- obj$X
  parameters <- obj$parameters

  plot(y_actual ~ y_pred ,xlim = range(y_pred), ylim = range(-.05, 1.5))

  x_line <- seq(min(y_pred), max(y_pred), length = length(y_pred))
  y_line <- pf(t(X), obj$betas)
  lines(x_line, sort(y_line), col = "red", lty = 1, lwd = 2)
}
```

Here we use the plot() function in this package
```{r logplot_abalone}
plot(results)

```

.
### *Bootstrap Confidence intervals*
  The user is able to choose (i) the significance level α to obtain for the 1−α confidence intervals for β, and (ii)    the number of bootstraps which by default is 20.
  
```{r bootstrap}
bootstrap_conf_intervals <- function(X, y, alpha = 0.05, n_bootstraps = 20) {
  n <- nrow(X)
  beta_bootstraps <- matrix(NA, ncol = n_bootstraps, nrow = length(log_reg(X, y)$betas))

  for (i in 1:n_bootstraps) {
    # Sample with replacement
    indices <- sample(1:n, replace = TRUE)
    X_bootstrap <- X[indices, ]
    y_bootstrap <- y[indices]

    # Run logistic regression on the bootstrap sample
    beta_bootstraps[, i] <- log_reg(X_bootstrap, y_bootstrap)$betas
  }

  # Calculate confidence intervals
  lower <- apply(beta_bootstraps, 1, function(row) quantile(row, alpha / 2))
  upper <- apply(beta_bootstraps, 1, function(row) quantile(row, 1 - alpha / 2))

  intervals <- data.frame(lower = lower, upper = upper)
  return(intervals)
}


```  

For example, using the Abalone dataset with different measurements as predictors: 

```{r bootstrap example}

# Bootstrap confidence intervals
intervals <- bootstrap_conf_intervals(X_abalone, y_abalone)

# Print the resulting intervals
print(intervals)
```

### *Generate resulting "Confusion Matrix”*

This uses a cut-off value for prediction at 0.5 (i.e. assign value 1 for predictions above 0.5 and value 0 for prediction below or equal to 0.5). Based on this cut-off value, it also outputs the following metrics:
Prevalence
Accuracy
Sensitivity
Specificity
False Discovery Rate
Diagnostic Odds Ratio

```{r confmat}
library(caret)
confmat <- function(y_actual, y_pred, cutoff = 0.5){
  #convert predicted values to 0 or 1 based on threshold
  y_actual <- factor(y_actual)
  y_bin <- factor(ifelse(y_pred >= cutoff, 1,0), levels = levels(y_actual))
  cmat <- confusionMatrix(y_actual, y_bin)
  metrics <- cmat$byClass
  prevalence <- as.numeric(metrics["Prevalence"])
  accuracy <- as.numeric(cmat$overall["Accuracy"])
  sensitivity <- as.numeric(metrics["Sensitivity"])
  specificity <- as.numeric(metrics["Specificity"])
  true_p <- cmat$byClass["Pos Pred Value"] * cmat$byClass["Precision"]
  false_p <- cmat$byClass["Neg Pred Value"] * cmat$byClass["Precision"]
  false_discovery_rate <- as.numeric(false_p / (false_p + true_p))
  diagnostic_odds_ratio <- as.numeric(metrics["Sensitivity"]/(1 - metrics["Specificity"]))
  
  out <- invisible(list("Prevalence" = prevalence, "Accuracy" = accuracy, "Sensitivity" = sensitivity, "Specificity" = specificity, "False Discovery Rate" = false_discovery_rate, "Diagnostic Odds Ratio" = diagnostic_odds_ratio))
  return(out)

}

```

Here is an example of the output when running the confusion matrix function on the Abalone data set with a cutoff of 0.5. We can use the outputs from the log_reg function to use as the parameters for this function. 
``` {r }
y_p <- results$y_pred
y_a <- results$y_actual

test <- confmat(y_a, y_p)
test

```

### *Plot above metrics over a grid*

The possibility for the user to plot of any of the above metrics evaluated over a grid of cut-off values for prediction going from 0.1 to 0.9 with steps of 0.1.

```{r metrics} 
plot_metrics <- function(X, y, cutoff_values = seq(0.1, 0.9, by = 0.1)) {
  result <- log_reg(X, y)
  predicted_probs <- pf(t(X), result$betas)
  metrics_matrix <- matrix(NA, nrow = length(cutoff_values), ncol = 7,
                           dimnames = list(NULL, c("Cutoff", "Prevalence", "Accuracy", "Sensitivity", "Specificity", "False Discovery Rate", "Diagnostic Odds Ratio")))

  # Calculate metrics for each cutoff
  for (i in seq_along(cutoff_values)) {
    metrics <- confmat(y, predicted_probs, cutoff_values[i])
    metrics_matrix[i, ] <- c(cutoff_values[i], metrics[[1]], metrics[[2]], metrics[[3]], metrics[[4]], metrics[[5]], metrics[[6]])
  }

  # Plot metrics
  par(mfrow = c(3, 2), mar = c(4, 4, 2, 1), oma = c(0, 0, 2, 0))
  metrics_names <- c("Prevalence", "Accuracy", "Sensitivity", "Specificity", "False Discovery Rate", "Diagnostic Odds Ratio")
  for (i in seq_along(metrics_names)) {
    plot(metrics_matrix[, "Cutoff"], metrics_matrix[, i + 1], type = "l", col = i + 1,
         xlab = "Cutoff", ylab = metrics_names[i], main = paste("Metric vs. Cutoff"))

  }
}

```
Here is an example of this function being used with the above calculated metrics from the Abalone data set. 
```{r}
plotmet <- plot_metrics(X_abalone, y_abalone)


```

### *Include Help documentation for all functions*


